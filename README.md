# local_rag_system
Lokales RAG-System mit Ollama und LangChain

## Voraussetzungen
- Python 3.11
- Ollama lokal installiert mit Modellen wie llama3, mistral, phi


## 1. Start Backend (Docker)

### 0 - Erstelle den Docker Container (sofern noch nicht vorhanden)
```
docker build -t lokales-rag-claude_v2 .
```

### 1A - Starte das Backend (Variante A - per Docker Compose)
```
docker-compose up
```

### 1B - Starte das Backend (Variante B - manuell)
```
docker run --add-host=host.docker.internal:host-gateway -v "./documents:/app/documents" -p 7860:7860 lokales-rag-claude_v2
```

## 2. Oberfl√§che aufrufen
Dann √∂ffne deinen Browser unter: http://localhost:7860

## Nutzung
1. W√§hle ein Modell aus (z.‚ÄØB. llama3)
2. Gib den Pfad zu deinen Dokumenten an (z.‚ÄØB. `./documents`)
3. Klicke auf "Laden"
4. Stelle eine Frage zu deinen Dokumenten

## Sonstiges


### Test, ob Ollama im Host erreichbar ist
```
curl http://localhost:11434/api/tags
```

### Nach √Ñnderungen, den Container neu bauen:
```
docker build -t lokales-rag .
```
-> Danach wieder mit run starten


### Docker auf Urzustand (Alle Container, Images, etc. l√∂schen - bis auf aktive)
```
docker system prune -a --volumes
```

### Alle Docker Container l√∂schen
Wenn zu viele verschiedene Docker Container mit Docker run gebildet wurden, k√∂nnen alle Docker Container mit diesem Befehl gel√∂scht werden:
```
docker builder prune --all --force
```

### Docker foo

#### Docker Compose nach √Ñnderungen

```
docker-compose down
```

```
docker-compose build --no-cache
```

```
docker-compose up
```

#### Docker
```
docker stop <container-id>
docker rm <container-id>
docker run -v ... lokales-rag
```

## Changehistory

### V2

#### Wichtigste √Ñnderungen

Diese √Ñnderungen bieten dir ein erheblich verbessertes RAG-System mit:

H√∂herer Antwortqualit√§t durch angepasste Prompts und optimierte Chain-Typen
Persistenter Speicherung von Embeddings mit Chroma
Einer intuitiveren Benutzeroberfl√§che mit mehr Funktionen
Besserer Verwaltung von Vektorspeichern f√ºr verschiedene Dokumentensammlungen

#### 1. In app/rag.py

- **Prompt-Templates**: Ich habe ein anpassbares Prompt-Template hinzugef√ºgt, das die Qualit√§t der Antworten verbessert, indem es explizite Anweisungen f√ºr das LLM enth√§lt.
- **Chain-Typen**: Die Funktion build_qa_chain unterst√ºtzt jetzt verschiedene Chain-Typen:
	- `stuff`: Standardmethode f√ºr kleinere Dokumentensammlungen
	- `map_reduce`: Besser f√ºr gro√üe Dokumentenmengen, verarbeitet jeden Chunk separat
	- `refine`: Iterativer Ansatz mit schrittweiser Verfeinerung der Antwort
	- `map_rerank`: Ordnet Antworten nach Relevanz

#### 2. In app/vectorstore.py

- **Chroma statt FAISS**: Ersetzt FAISS durch Chroma, das eine persistente Speicherung erm√∂glicht
- **Bessere Vektorstore-Verwaltung**:
	- Funktionen zum Auflisten, Laden und L√∂schen von Vektorspeichern
	- Verbesserte Fehlerbehandlung und R√ºckgabewerte
	- Strukturierte Verzeichnisorganisation f√ºr Vektorspeicher

#### 3. In app/ui.py

- **Tab-basierte UI**: Trennung von Einrichtung und Fragebereich f√ºr bessere √úbersicht
- **Chain-Typ-Auswahl**: Dropdown zur Auswahl des gew√ºnschten Chain-Typs
- **Vektorspeicher-Management**:
	- Anzeige und Auswahl vorhandener Vektorspeicher
	- Buttons zum Laden und L√∂schen von Vektorspeichern
	- Refresh-Funktion f√ºr die Vektorspeicher-Liste
- **Verbesserte Statusanzeigen**: Ausf√ºhrlichere Informationen √ºber den aktuellen Zustand des Systems

#### 4. In requirements.txt

- **Aktualisierte Abh√§ngigkeiten**:
	- Hinzugef√ºgt: `chromadb` f√ºr persistente Vektorspeicherung
	- Versionsangaben f√ºr bessere Kompatibilit√§t
	- `sentence-transformers` als optionale Alternative zu Ollama-Embeddings

---

**Hinweis**: Es wird vorausgesetzt, dass Ollama korrekt l√§uft und die Modelle geladen werden k√∂nnen. Die Modelle m√ºssen vorher √ºber `ollama pull` heruntergeladen werden.

Viel Spa√ü beim lokalen Fragenstellen! ü§ì