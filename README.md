# local_rag_system
Lokales RAG-System mit Ollama und LangChain

## Voraussetzungen
- Python 3.11
- Ollama lokal installiert mit Modellen wie llama3, mistral, phi

## Start (lokal auf Windows ohne Docker)
```
pip install -r requirements.txt
python app/main.py
```

## Start mit Docker (Backend)
```
docker build -t lokales-rag .
docker run -v "${PWD.Path}/documents:/app/documents" -p 7860:7860 lokales-rag
docker run --name rag-system -v "${PWD.Path}/documents:/app/documents" -p 7860:7860 lokales-rag

```

Dann √∂ffne deinen Browser unter: http://localhost:7860

## Nutzung
1. W√§hle ein Modell aus (z.‚ÄØB. llama3)
2. Gib den Pfad zu deinen Dokumenten an (z.‚ÄØB. `./documents`)
3. Klicke auf "Laden"
4. Stelle eine Frage zu deinen Dokumenten

## Sonstiges


### Test, ob Ollama im Host erreichbar ist
```
curl http://localhost:11434/api/tags
```

### Nach √Ñnderungen, den Container neu bauen:
```
docker build -t lokales-rag .
```
-> Danach wieder mit run starten


### Docker auf Urzustand (Alle Container, Images, etc. l√∂schen - bis auf aktive)
```
docker system prune -a --volumes
```

### Alle Docker Container l√∂schen
Wenn zu viele verschiedene Docker Container mit Docker run gebildet wurden, k√∂nnen alle Docker Container mit diesem Befehl gel√∂scht werden:
```
docker builder prune --all --force
```

### Docker foo

```
docker stop <container-id>
docker rm <container-id>
docker run -v ... lokales-rag
```
---

**Hinweis**: Es wird vorausgesetzt, dass Ollama korrekt l√§uft und die Modelle geladen werden k√∂nnen. Die Modelle m√ºssen vorher √ºber `ollama pull` heruntergeladen werden.

Viel Spa√ü beim lokalen Fragenstellen! ü§ì