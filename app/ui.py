# app/ui.py - Erweiterte Version mit Token-Tracking und robuster Ollama-Verbindung

import os
import datetime
import gradio as gr
import threading
import time
import re
import html
import markdown
from markdown.extensions import codehilite, fenced_code, tables, toc
import bleach

from app.loader import load_documents_from_path
from app.vectorstore import (
    build_vectorstore, load_vectorstore, delete_vectorstore,
    list_vectorstores
)
from app.rag import build_qa_chain, get_chain_type_description
from app.config import config
from app.connection_utils import (
    check_ollama_connection_with_retry, 
    wait_for_ollama_ready, 
    get_ollama_status
)
from app.token_tracker import token_tracker

# === Globale Variablen ===
qa_chain = None
current_vectorstore = None
current_model = None
current_chain_type = "stuff"
_ollama_status_cache = {'connected': False, 'last_check': 0, 'cache_duration': 10}

# === Sichere Markdown-Verarbeitung ===

# Erlaubte HTML-Tags und Attribute f√ºr Bleach
ALLOWED_TAGS = [
    'p', 'br', 'strong', 'b', 'em', 'i', 'u', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
    'ul', 'ol', 'li', 'blockquote', 'code', 'pre', 'a', 'table', 'thead', 'tbody',
    'tr', 'td', 'th', 'div', 'span', 'hr'
]

ALLOWED_ATTRIBUTES = {
    'a': ['href', 'title'],
    'code': ['class'],
    'pre': ['class'],
    'div': ['class'],
    'span': ['class'],
    'table': ['class'],
    'th': ['align'],
    'td': ['align']
}

def sanitize_and_format_text(text):
    """
    Konvertiert Markdown zu HTML und sanitized das Ergebnis f√ºr sichere Anzeige.
    
    Args:
        text (str): Der zu formatierende Text (kann Markdown enthalten)
        
    Returns:
        str: Sicherer HTML-Text f√ºr die Anzeige
    """
    if not text or not isinstance(text, str):
        return ""
    
    try:
        # 1. Markdown zu HTML konvertieren
        md = markdown.Markdown(
            extensions=[
                'codehilite',
                'fenced_code', 
                'tables',
                'toc',
                'nl2br'  # Zeilenumbr√ºche beibehalten
            ],
            extension_configs={
                'codehilite': {
                    'css_class': 'highlight',
                    'use_pygments': False  # Vermeidet externe Abh√§ngigkeiten
                }
            }
        )
        
        html_content = md.convert(text)
        
        # 2. HTML sanitizen (entfernt potentiell sch√§dliche Inhalte)
        clean_html = bleach.clean(
            html_content,
            tags=ALLOWED_TAGS,
            attributes=ALLOWED_ATTRIBUTES,
            strip=True  # Entfernt unerlaubte Tags komplett
        )
        
        # 3. Zus√§tzliche Bereinigung f√ºr bessere Darstellung
        clean_html = clean_html.replace('\n\n', '\n')  # Doppelte Zeilenumbr√ºche reduzieren
        
        return clean_html
        
    except Exception as e:
        # Fallback: Text escapen falls Markdown-Verarbeitung fehlschl√§gt
        print(f"Markdown-Verarbeitung fehlgeschlagen: {e}")
        return html.escape(text).replace('\n', '<br>')

def format_answer_with_sources_and_tokens(answer, sources, model_info, token_info):
    """
    Formatiert die Antwort mit Quellen, Modellinformationen und Token-Details als HTML.
    
    Args:
        answer (str): Die Hauptantwort
        sources (list): Liste der Quelldateien
        model_info (str): Informationen √ºber das verwendete Modell
        token_info (str): Token-Verbrauchsinformationen
        
    Returns:
        str: Formatierte HTML-Antwort
    """
    # Hauptantwort formatieren
    formatted_answer = sanitize_and_format_text(answer)
    
    # Token-Informationen formatieren (Markdown zu HTML)
    formatted_token_info = sanitize_and_format_text(token_info)
    
    # Quellen-Sektion
    sources_html = ""
    if sources:
        unique_sources = list(set(sources))
        sources_list = []
        for source in unique_sources:
            # Nur den Dateinamen anzeigen, nicht den vollst√§ndigen Pfad
            filename = os.path.basename(source)
            sources_list.append(f"<li><code>{html.escape(filename)}</code></li>")
        
        sources_html = f"""
<hr>
<h4>üìö Quellen:</h4>
<ul>
{''.join(sources_list)}
</ul>
"""
    
    # Token-Sektion
    token_html = f"""
<hr>
<div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007acc;">
{formatted_token_info}
</div>
"""
    
    # Modell-Info
    model_html = f"""
<hr>
<p><small><strong>ü§ñ Modell:</strong> {html.escape(model_info)}</small></p>
"""
    
    return f"{formatted_answer}{sources_html}{token_html}{model_html}"

# === Verbesserte Verbindungspr√ºfung ===

def get_cached_ollama_status():
    """
    Cached Ollama-Status um h√§ufige Netzwerk-Calls zu vermeiden
    """
    current_time = time.time()
    
    # Cache f√ºr 10 Sekunden
    if (current_time - _ollama_status_cache['last_check']) < _ollama_status_cache['cache_duration']:
        return _ollama_status_cache['connected']
    
    # Status aktualisieren
    status = get_ollama_status()
    _ollama_status_cache['connected'] = status['connected']
    _ollama_status_cache['last_check'] = current_time
    
    return status['connected']

def check_ollama_with_user_feedback():
    """
    Ollama-Verbindung mit Benutzer-Feedback pr√ºfen
    """
    status_info = get_ollama_status()
    
    if not status_info['connected']:
        # Retry-Versuch
        if check_ollama_connection_with_retry(max_retries=3, delay=1.0):
            return True, "‚úÖ Verbindung zu Ollama hergestellt"
        else:
            return False, f"‚ùå {status_info['status_message']}\nüí° Tipp: Stelle sicher, dass Ollama l√§uft und erreichbar ist"
    
    return True, status_info['status_message']

# === Token-Statistik-Funktionen ===

def get_session_stats():
    """Gibt Session-Statistiken zur√ºck"""
    stats = token_tracker.get_session_summary()
    
    return f"""üìä **Session-Statistiken:**
‚Ä¢ Anfragen: {stats['total_requests']}
‚Ä¢ Gesamt-Token: {stats['total_tokens']:,}
  - Input: {stats['total_input_tokens']:,}
  - Output: {stats['total_output_tokens']:,}
‚Ä¢ √ò Token/Anfrage: {stats['average_tokens_per_request']}
‚Ä¢ √ò Verarbeitungszeit: {stats['average_processing_time']}s
‚Ä¢ Session-Dauer: {stats['session_duration']}
‚Ä¢ Gesch√§tzte Gesamtkosten: ${stats['estimated_total_cost']}"""

def get_recent_requests_table():
    """Erstellt eine HTML-Tabelle mit den letzten Anfragen"""
    recent = token_tracker.get_recent_requests(10)
    
    if not recent:
        return "<p><em>Noch keine Anfragen in dieser Session.</em></p>"
    
    table_rows = []
    for req in recent:
        table_rows.append(f"""
        <tr>
            <td>{req['timestamp']}</td>
            <td style="max-width: 200px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;" title="{html.escape(req['question'])}">{html.escape(req['question'][:50])}...</td>
            <td>{req['input_tokens']:,}</td>
            <td>{req['output_tokens']:,}</td>
            <td><strong>{req['total_tokens']:,}</strong></td>
            <td>{req['processing_time']}s</td>
            <td>${req['cost_estimate']}</td>
        </tr>
        """)
    
    return f"""
    <table style="width: 100%; border-collapse: collapse; font-size: 12px;">
        <thead>
            <tr style="background-color: #f8f9fa;">
                <th style="border: 1px solid #ddd; padding: 8px;">Zeit</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Frage</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Input</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Output</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Gesamt</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Zeit</th>
                <th style="border: 1px solid #ddd; padding: 8px;">Kosten</th>
            </tr>
        </thead>
        <tbody>
            {''.join(table_rows)}
        </tbody>
    </table>
    """

def reset_token_stats():
    """Setzt die Token-Statistiken zur√ºck"""
    token_tracker.reset_session()
    return (
        get_session_stats(),
        get_recent_requests_table(),
        "üîÑ Token-Statistiken wurden zur√ºckgesetzt."
    )

# === Hilfsfunktionen ===

def create_vectorstore_name():
    """Erstellt einen eindeutigen Namen f√ºr einen neuen Vektorspeicher."""
    return f"vectorstore_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"

def get_vectorstore_base_path():
    """Gibt den Basispfad f√ºr Vektorspeicher zur√ºck."""
    return os.path.join(config.base_path, ".vectorstores")

def get_available_vectorstores():
    """Gibt eine Liste aller verf√ºgbaren Vektorspeicher zur√ºck."""
    vs_base_dir = get_vectorstore_base_path()
    vectorstores_dict = list_vectorstores(vs_base_dir)
    return list(vectorstores_dict.keys())

def update_vectorstore_dropdown_choices():
    """Aktualisiert die Dropdown-Auswahlm√∂glichkeiten f√ºr Vektorspeicher."""
    available_vs = get_available_vectorstores()
    
    if not available_vs:
        return gr.update(choices=[], value=None), "‚ÑπÔ∏è Kein Vektorspeicher gefunden."
    
    return (
        gr.update(choices=available_vs, value=available_vs[0]), 
        f"‚úÖ {len(available_vs)} Vektorspeicher gefunden."
    )

# === Model Loading ===

def load_model_only(model, chain_type):
    """L√§dt nur das Modell ohne Dokumente oder Vektorspeicher."""
    global current_model, current_chain_type
    
    connected, status_msg = check_ollama_with_user_feedback()
    if not connected:
        return f"‚ö†Ô∏è {status_msg}"
    
    try:
        current_model = model
        current_chain_type = chain_type
        
        return f"""‚úÖ Modell '{model}' mit Chain-Typ '{chain_type}' geladen
‚ÑπÔ∏è {get_chain_type_description(chain_type)}
‚ö†Ô∏è Hinweis: Zum Fragen beantworten muss noch ein Vektorspeicher geladen werden."""
        
    except Exception as e:
        return f"‚ö†Ô∏è Fehler beim Laden des Modells: {str(e)}"

# === Setup & Laden ===

def setup_qa(model, doc_path, chain_type, vectorstore_name):
    """L√§dt Dokumente und erstellt einen neuen Vektorspeicher mit verbesserter Verbindungspr√ºfung."""
    global qa_chain, current_vectorstore, current_model, current_chain_type
    current_model, current_chain_type = model, chain_type

    # Verbesserte Verbindungspr√ºfung
    connected, status_msg = check_ollama_with_user_feedback()
    if not connected:
        return (
            f"‚ö†Ô∏è {status_msg}",
            gr.update(),
            gr.update()
        )

    try:
        vs_path = os.path.join(get_vectorstore_base_path(), vectorstore_name)
        docs, loaded_files = load_documents_from_path(doc_path)

        if not docs:
            return (
                "‚ö†Ô∏è Keine unterst√ºtzten Dokumente gefunden",
                gr.update(),
                gr.update()
            )

        current_vectorstore = build_vectorstore(docs, model, vs_path)
        qa_chain = build_qa_chain(current_vectorstore, model, chain_type)

        # Nach erfolgreichem Erstellen: Dropdown aktualisieren
        available_vs = get_available_vectorstores()
        
        success_msg = f"""‚úÖ Setup erfolgreich abgeschlossen!
ü§ñ Modell: {model} (Chain-Typ: {chain_type})
‚ÑπÔ∏è {get_chain_type_description(chain_type)}
üìö {len(docs)} Dokumentenabschnitte aus {len(loaded_files)} Dateien verarbeitet
üíæ Vektorspeicher '{vectorstore_name}' gespeichert"""

        return (
            success_msg,
            gr.update(choices=available_vs, value=vectorstore_name),
            f"‚úÖ Vektorspeicher '{vectorstore_name}' wurde erstellt und geladen."
        )
    
    except Exception as e:
        return (
            f"‚ö†Ô∏è Fehler: {str(e)}",
            gr.update(),
            gr.update()
        )

def load_existing_vectorstore(model, selection, chain_type):
    """L√§dt einen vorhandenen Vektorspeicher mit verbesserter Verbindungspr√ºfung."""
    global qa_chain, current_vectorstore, current_model, current_chain_type
    current_model, current_chain_type = model, chain_type

    if not selection:
        return "‚ö†Ô∏è Bitte w√§hle einen Vektorspeicher aus"

    connected, status_msg = check_ollama_with_user_feedback()
    if not connected:
        return f"‚ö†Ô∏è {status_msg}"

    try:
        vs_base_dir = get_vectorstore_base_path()
        vectorstores = list_vectorstores(vs_base_dir)
        path = vectorstores.get(selection)

        if not path:
            return f"‚ö†Ô∏è Vektorspeicher '{selection}' nicht gefunden"

        current_vectorstore = load_vectorstore(path, model)
        qa_chain = build_qa_chain(current_vectorstore, model, chain_type)

        return f"""‚úÖ Vektorspeicher '{selection}' erfolgreich geladen
ü§ñ Modell '{model}' mit Chain-Typ '{chain_type}' initialisiert
‚ÑπÔ∏è {get_chain_type_description(chain_type)}"""

    except Exception as e:
        return f"‚ö†Ô∏è Fehler beim Laden: {str(e)}"

# === System-Status-Check ===

def get_system_status():
    """Gibt aktuellen System-Status zur√ºck"""
    status_info = get_ollama_status()
    
    ollama_status = status_info['status_message']
    
    if current_model and qa_chain:
        qa_status = f"‚úÖ QA-System bereit (Modell: {current_model})"
    elif current_model:
        qa_status = "‚ö†Ô∏è Modell geladen, aber kein Vektorspeicher"
    else:
        qa_status = "‚≠ï Kein Modell geladen"
    
    return f"{ollama_status}\n{qa_status}"

def refresh_system_status():
    """Aktualisiert den System-Status"""
    # Cache leeren f√ºr frischen Status
    _ollama_status_cache['last_check'] = 0
    return get_system_status()

# === Vektorspeicher-Verwaltung ===

def delete_selected_vectorstore(selection):
    """L√∂scht den ausgew√§hlten Vektorspeicher und aktualisiert die Dropdown-Liste."""
    if not selection:
        return (
            "‚ö†Ô∏è Bitte einen Vektorspeicher ausw√§hlen",
            gr.update(),
            gr.update()
        )

    vs_base_dir = get_vectorstore_base_path()
    vectorstores = list_vectorstores(vs_base_dir)
    path = vectorstores.get(selection)

    if not path:
        return (
            f"‚ö†Ô∏è Vektorspeicher '{selection}' nicht gefunden",
            gr.update(),
            gr.update()
        )

    try:
        if delete_vectorstore(path):
            available_vs = get_available_vectorstores()
            
            if available_vs:
                new_selection = available_vs[0]
                dropdown_update = gr.update(choices=available_vs, value=new_selection)
                vector_status = f"‚úÖ '{selection}' gel√∂scht. Auswahl: '{new_selection}'"
            else:
                dropdown_update = gr.update(choices=[], value=None)
                vector_status = "‚ÑπÔ∏è Alle Vektorspeicher gel√∂scht."
            
            return (
                f"‚úÖ Vektorspeicher '{selection}' erfolgreich gel√∂scht",
                dropdown_update,
                vector_status
            )
        else:
            return (
                f"‚ö†Ô∏è Fehler beim L√∂schen von '{selection}'",
                gr.update(),
                gr.update()
            )
    except Exception as e:
        return (
            f"‚ö†Ô∏è Fehler beim L√∂schen: {str(e)}",
            gr.update(),
            gr.update()
        )

def refresh_vectorstore_list():
    """Aktualisiert explizit die Vektorspeicher-Liste."""
    return update_vectorstore_dropdown_choices()

# === Fragen & Antworten ===

def ask_question(question):
    """Verarbeitet eine Frage √ºber die geladenen Dokumente mit formatierter HTML-Ausgabe und Token-Tracking."""
    global qa_chain, current_model, current_chain_type

    if qa_chain is None:
        return (
            "‚ö†Ô∏è Bitte zuerst Modell und Vektorspeicher laden.",
            get_session_stats(),
            get_recent_requests_table()
        )

    if not question.strip():
        return (
            "‚ö†Ô∏è Bitte eine Frage eingeben.",
            get_session_stats(),
            get_recent_requests_table()
        )

    try:
        result = qa_chain.invoke({"query": question})
        answer = result['result']
        
        # Quellen extrahieren
        sources = []
        for doc in result.get('source_documents', []):
            if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                sources.append(doc.metadata['source'])
        
        # Modellinformationen
        model_info = f"{current_model}, Chain-Typ: {current_chain_type}"
        
        # Token-Informationen aus dem Ergebnis
        token_info = result.get('token_info', 'Token-Informationen nicht verf√ºgbar')
        
        # Formatierte Antwort mit HTML erstellen
        formatted_answer = format_answer_with_sources_and_tokens(
            answer, sources, model_info, token_info
        )
        
        # Token-Statistiken aktualisieren
        session_stats = get_session_stats()
        recent_table = get_recent_requests_table()
        
        return formatted_answer, session_stats, recent_table

    except Exception as e:
        error_msg = f"‚ö†Ô∏è Fehler bei der Frageverarbeitung: {str(e)}"
        return (
            html.escape(error_msg),
            get_session_stats(),
            get_recent_requests_table()
        )

# === Benutzeroberfl√§che ===

def create_interface():
    """Erstellt die Gradio-Benutzeroberfl√§che."""
    
    with gr.Blocks(theme=gr.themes.Soft(), title="Lokales RAG System") as demo:
        gr.Markdown("# ü§ñ Lokales RAG mit Ollama und LangChain")

        with gr.Tab("Einrichtung"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### Modell-Konfiguration")
                    model = gr.Dropdown(
                        choices=config.get_available_models(),
                        label="Modellwahl",
                        value=config.get_available_models()[0] if config.get_available_models() else None
                    )
                    chain_type = gr.Dropdown(
                        choices=["stuff", "map_reduce", "refine", "map_rerank"],
                        label="Chain-Typ", 
                        value="stuff"
                    )
                    
                    load_model_btn = gr.Button("ü§ñ Modell laden", variant="secondary")
                    
                    gr.Markdown("### Neue Dokumente laden")
                    doc_path = gr.Textbox(
                        label="Pfad zu Dokumenten", 
                        value=config.get_documents_path()
                    )
                    vectorstore_name = gr.Textbox(
                        label="Name f√ºr neuen Vektorspeicher", 
                        value=create_vectorstore_name()
                    )
                    
                    load_btn = gr.Button("üìö Dokumente laden & Vektorspeicher erstellen", variant="primary")

                with gr.Column(scale=1):
                    gr.Markdown("### System-Status")
                    
                    status = gr.Textbox(
                        label="Status-Meldungen", 
                        lines=6,
                        value="Initialisiere System..."
                    )
                    
                    # Button zum manuellen Status-Refresh
                    refresh_status_btn = gr.Button("üîÑ Status aktualisieren", size="sm")
                    
                    gr.Markdown("### Vektorspeicher-Verwaltung")
                    vectorstore_list = gr.Dropdown(
                        label="Vorhandene Vektorspeicher",
                        choices=[],
                        value=None,
                        interactive=True
                    )
                    
                    vector_status = gr.Textbox(
                        label="Vektorspeicher-Status", 
                        interactive=False,
                        value="Lade Vektorspeicher-Liste..."
                    )
                    
                    with gr.Row():
                        refresh_btn = gr.Button("üîÑ Liste aktualisieren")
                        load_vs_btn = gr.Button("üìÇ Vektorspeicher laden")
                        delete_vs_btn = gr.Button("üóëÔ∏è L√∂schen", variant="stop")

        with gr.Tab("Fragen und Antworten"):
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("### Dokumente befragen")
                    question = gr.Textbox(
                        label="Frage an die Dokumente", 
                        placeholder="Was ist der Hauptinhalt der Dokumente?",
                        lines=2
                    )
                    ask_btn = gr.Button("üîç Frage stellen", variant="primary")
                    
                    # HTML-Komponente f√ºr formatierte Antworten verwenden
                    answer = gr.HTML(
                        label="Antwort",
                        value="<p><em>Hier erscheint die formatierte Antwort mit Token-Informationen...</em></p>"
                    )
                
                with gr.Column(scale=1):
                    gr.Markdown("### üìä Token-Statistiken")
                    
                    session_stats = gr.HTML(
                        label="Session-√úbersicht",
                        value="<p><em>Noch keine Statistiken verf√ºgbar.</em></p>"
                    )
                    
                    reset_stats_btn = gr.Button("üîÑ Statistiken zur√ºcksetzen", size="sm")
                    
                    gr.Markdown("### üìà Letzte Anfragen")
                    recent_requests = gr.HTML(
                        label="Anfrage-Historie",
                        value="<p><em>Noch keine Anfragen.</em></p>"
                    )

        with gr.Tab("Token-Analytics"):
            gr.Markdown("### üìä Detaillierte Token-Analyse")
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### Session-√úbersicht")
                    detailed_stats = gr.HTML(
                        value="<p><em>Starte eine Unterhaltung, um Statistiken zu sehen.</em></p>"
                    )
                
                with gr.Column():
                    gr.Markdown("#### Anfrage-Historie")
                    detailed_history = gr.HTML(
                        value="<p><em>Anfrage-Historie wird hier angezeigt.</em></p>"
                    )
            
            refresh_analytics_btn = gr.Button("üîÑ Analytics aktualisieren")

        # === Event-Handler ===
        
        # Status aktualisieren
        refresh_status_btn.click(
            fn=refresh_system_status,
            inputs=[],
            outputs=[status]
        )
        
        # Modell laden
        load_model_btn.click(
            fn=load_model_only,
            inputs=[model, chain_type],
            outputs=[status]
        )
        
        # Dokumente laden und neuen Vektorspeicher erstellen
        load_btn.click(
            fn=setup_qa,
            inputs=[model, doc_path, chain_type, vectorstore_name],
            outputs=[status, vectorstore_list, vector_status]
        ).then(
            fn=lambda: create_vectorstore_name(),
            inputs=[],
            outputs=[vectorstore_name]
        )
        
        # Vektorspeicher-Liste aktualisieren
        refresh_btn.click(
            fn=refresh_vectorstore_list,
            inputs=[],
            outputs=[vectorstore_list, vector_status]
        )
        
        # Vorhandenen Vektorspeicher laden
        load_vs_btn.click(
            fn=load_existing_vectorstore,
            inputs=[model, vectorstore_list, chain_type],
            outputs=[status]
        )
        
        # Vektorspeicher l√∂schen
        delete_vs_btn.click(
            fn=delete_selected_vectorstore,
            inputs=[vectorstore_list],
            outputs=[status, vectorstore_list, vector_status]
        )
        
        # Frage stellen
        ask_btn.click(
            fn=ask_question,
            inputs=[question],
            outputs=[answer, session_stats, recent_requests]
        )
        
        question.submit(
            fn=ask_question,
            inputs=[question],
            outputs=[answer, session_stats, recent_requests]
        )
        
        # Token-Statistiken zur√ºcksetzen
        reset_stats_btn.click(
            fn=reset_token_stats,
            inputs=[],
            outputs=[session_stats, recent_requests, status]
        )
        
        # Analytics aktualisieren
        refresh_analytics_btn.click(
            fn=lambda: (get_session_stats(), get_recent_requests_table()),
            inputs=[],
            outputs=[detailed_stats, detailed_history]
        )

        # Beim Laden der Seite: Initialisierung mit Verz√∂gerung
        demo.load(
            fn=lambda: (
                refresh_system_status(), 
                *update_vectorstore_dropdown_choices(),
                get_session_stats(),
                get_recent_requests_table()
            ),
            inputs=[],
            outputs=[status, vectorstore_list, vector_status, session_stats, recent_requests]
        )

    return demo

def start_ui():
    """Startet die Benutzeroberfl√§che mit Ollama-Warteschleife."""
    # Optional: Warte auf Ollama beim Start
    if not wait_for_ollama_ready(max_wait_time=10):
        print("‚ö†Ô∏è Warnung: Ollama scheint nicht bereit zu sein, starte UI trotzdem...")
    
    demo = create_interface()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)